{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD1-Timeseries-Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nTrouvain/TD1-Timeseries-Analysis/blob/main/TD1_Timeseries_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bCKE6Ppxc40"
      },
      "source": [
        "# TD1: Timeseries analysis using autoregressive methods and general Box-Jenkins methods\n",
        "\n",
        "Some useful translations, just in case:\n",
        "\n",
        "- **a timeseries**: une série temporelle (always plural in English)\n",
        "- **a trend**: une tendance\n",
        "- **a lag**: un retard, un décalage dans le temps\n",
        "- **stationary**: stationnaire\n",
        "\n",
        "\n",
        "Some interesting content to dive deeper and/or go further about timeseries analysis, or that might help you during the TD:\n",
        "\n",
        "- [The engineering statistics handbook on timeseries analysis](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm)\n",
        "\n",
        "- [A Stanford class on autoregressive models seen as generative models](https://deepgenerativemodels.github.io/notes/) (and more on deep generative models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r3eKuTIxv1s"
      },
      "source": [
        "!pip install statsmodels==0.12.1\n",
        "!pip install sktime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3CDTGeixYqV"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels\n",
        "import sktime\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DyHgOIIV-A-"
      },
      "source": [
        "## 1. Analysis\n",
        "\n",
        "For this exercise, we will use a timeseries representing daily average temperature in Melbourne, Australia between 1980 and 1990.\n",
        "\n",
        "This timeseries will be stored in a [Pandas DataFrame object](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), a standard to handle tabular data in Python.\n",
        "\n",
        "This analysis will follow the steps proposed by George Box and Gwilym Jenkins in 1970, called [Box-Jenkins method](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method), which emphasizes issues encountered when appliying autoregressive methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwIn26ETxdoi"
      },
      "source": [
        "# Read data from remote repository\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EdqqTlzykqY"
      },
      "source": [
        "# Display the 5 first data points\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp9qJsRLXW0d"
      },
      "source": [
        "### 1.1 Run-plots analysis\n",
        "\n",
        "\"Run-plots\" are the simplest representation of a timeseries, where the x-axis represents time and the y-axis represents the observed variable, here temperature in Celsius degrees.\n",
        "\n",
        "\n",
        "**Question: Given the figures and the statistic test below, what hypothesis can you draw regarding the behaviour of this timeseries? Is is stationary? Does it displays seasonality? Trending? Explain. You can create additional figures if you need.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmFNPA0yYWK8"
      },
      "source": [
        "***(You answer here)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CHeMxPoylNz"
      },
      "source": [
        "# Plot the full timeseries\n",
        "df.plot(figsize=(20, 4), title=\"Temperature in Melbourne - 1980 to 1990\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTkxFN0KVppU"
      },
      "source": [
        "# Plot the first year of data\n",
        "df.iloc[:365].plot(figsize=(20, 4), title=\"Temperature in Melbourne - one year\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dPU4x5ItJWm"
      },
      "source": [
        "The Augmented Dickey-Fuller test is a statistical test used to check\n",
        "the stationarity of a timeseries. It is implemented in the `adfuller()` function in `statsmodels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPHEs26Bt1Kg"
      },
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "adf, p, *other_stuff = adfuller(df)\n",
        "\n",
        "print(f\"p-value (95% confidence interval): {p:g}, statistics: {adf:g}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptoWAiHvbaRh"
      },
      "source": [
        "### 1.2 Autocorrelation and partial autocorrelation\n",
        "\n",
        "Autocorrelation (and partial autocorrelation) are metrics that can be computed to evaluate **how dependent the variable is from its $n$ previous values**, what is called a **lag (of length n)**.\n",
        "\n",
        "**Question: Plot $X[t-1]$ versus $X[t]$, for all $t$. What can you conclude on the autocorrelation of the series with a lag of 1? You can also compute the Pearson correlation coefficient between $X[t-1]$ and $X[t]$.**\n",
        "\n",
        "*Some help:*\n",
        "\n",
        "- You can create a new DataFrame with all values shifted from $n$ timestep using the `.shift()` method of the DataFrame. See Pandas documentation for more.\n",
        "\n",
        "- You can plot some data versius some other using the `plt.scatter(X, Y)` method of Matplotlib. This plots a dot on the figure for every couple `(x, y)`\n",
        "in `X` and `Y`. See Matplotlib documentation for more.\n",
        "\n",
        "- Pearson coefficient can be computed using the DataFrame `.corr()` method. This method computes the correlation coefficient between all variables (*columns*) in the DataFrame. Try appliying this method to a DataFrame where one column is $X[t]$ and another column is the shifted timeseries $X[t-1]$. Note that you can merge two DataFrames into one using the function `pd.concat(dataframes)` of Pandas. See Pandas documentation for more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWQI87morti5"
      },
      "source": [
        "# Create a shifted version of the timeseries:\n",
        "df_shifted = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOj2L2Bir4_k"
      },
      "source": [
        "# Plot df vs. df_shifted\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "...\n",
        "\n",
        "plt.xlabel(\"X[t]\")\n",
        "plt.ylabel(\"X[t-1]\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quKscOBav3hR"
      },
      "source": [
        "***(Draw your conclusions here)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlGW9VT0tTtj"
      },
      "source": [
        "**Pearson correlation coefficient**\n",
        "\n",
        "To compute this coefficient, we first need to ensure that our variable follows a normal distribution. Let's plot the distribution, using the `.hist()` method of DataFrame objects:\n",
        "\n",
        "*(Optional)* Perform a normality test, using `scipy.stats.normaltest`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw0HtCqYtmmX"
      },
      "source": [
        "# Plot of the distribution of the variable\n",
        "# (in our case, the temperature histogram)\n",
        "\n",
        "...\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbkbyoJavHfK"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Normality test\n",
        "k2, p = ...\n",
        "\n",
        "print(f\"p-value (95% confidence interval): {p[0]:g}, statistics: {k2[0]:g}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6XH6cQfsIgK"
      },
      "source": [
        "# (Optional) Compute Pearson coefficients\n",
        " \n",
        "# First, concatenate df and df_shifted in df_all, following axis 1\n",
        "# (concatenate columns, not rows !)\n",
        "df_all = ...\n",
        "\n",
        "# Rename the columns\n",
        "df_all.columns = [\"X[t]\", \"X[t-1]\"]\n",
        "\n",
        "# Compute correlation and print it\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOJcrmHit8Fx"
      },
      "source": [
        "***(Draw your conclusions here)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUt8RA8bZ-RT"
      },
      "source": [
        "---\n",
        "We will now compute autocorrelation function (ACF) and partial autocorrelation function (PACF) of the timeseries. These functions compute correlation (or partial correlation) between $X[t]$ and $X[t-n]$, for an interval of different lags $n$. For now, we only evaluated correlation for lag $n=1$.\n",
        "\n",
        "**Question: Plot the ACF and the PACF of the timeseries, with $n={1, \\dots, 31}$ (one month lag) and $n={1, \\dots, 730}$ (2 years lag). What is your hypothesis on the lag to use to create the model ?**\n",
        "\n",
        "\n",
        "*Some help:*\n",
        "\n",
        "- See documentation of `statsmodels.graphics.tsaplots.plot_acf` to understand how to change the number of lags to plot.\n",
        "\n",
        "- **Autocorrelation** is the result of the multiplication (or convolution) of all points of the signal with themselves, shifted in time by a lag of $n$. The **autocorrelation function** (ACF) is the function giving autocorrelation for any lag $n$.\n",
        "\n",
        "- **Partial autocorrelation** is similar to autocorrelation, but the correlation between two points of the signal is computed assuming that this two points are independent from all points between them in time.  The **partial autocorrelation function** (PACF) is the function giving partial autocorrelation for any lag $n$.\n",
        "\n",
        "- Autocorrelation is helpful to check if a process in autoregressive. **Autoregressive processes are auto-correlated**.\n",
        "\n",
        "- Partial autocorrelation is helpful to find the order of an autoregressive process, i.e. **how many past steps are needed to predict the future one**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97v2Pxe7nTDQ"
      },
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvfaezR3o9aa"
      },
      "source": [
        "#### 1.2.1 Autocorrelation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l1DuRFidoDi"
      },
      "source": [
        "# Plot autocorrelation for lags between 1 and 730 days\n",
        "...\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkcqShKKc0Vr"
      },
      "source": [
        "# Plot autocorrelation for lags between 1 and 31 days\n",
        "...\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiGnsLyddre2"
      },
      "source": [
        "#### 1.2.2 Partial autocorrelation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbRJWYkLdxb5"
      },
      "source": [
        "# Plot partial autocorrelation for lags between 1 and 730 days\n",
        "...\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMTa1gZkduh5"
      },
      "source": [
        "# Plot partial autocorrelation for lags between 1 and 31 days\n",
        "...\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi49FNiqd3KB"
      },
      "source": [
        "***(Your hypothesis here)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjw_NyQ3eMAx"
      },
      "source": [
        "## 2. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY80DT8S1WFO"
      },
      "source": [
        "### 2.0 Modeling: AR from scratch (just as an example, nothing to do here)\n",
        "\n",
        "AR stands for AutoRegressive. Autoregressive models describe the value of any points in a timeseries given the values of $p$ previous points, establishing a linear relashionship between them such that:\n",
        "\n",
        "$$\n",
        "X_t = \\alpha + \\beta_1 X_{t-1} + \\beta_2 X_{t-2} + ... + \\beta_{p} X_{t-p} + \\epsilon_t\n",
        "$$\n",
        "\n",
        "where $X$ is a timeseries, $p$ is the lag used in the AR model, also called the **order** of the model, and $\\beta=\\{\\beta, \\dots, \\beta_p\\}$ and $\\alpha$ are the parameters we want to estimate. $\\epsilon_t$ is a white noise random process that we will consider to be 0 for all time steps in our model.\n",
        "\n",
        "$X_t$ is therefore linearly dependent from its $p$ previous values $X_{t-1}, \\dots, X_{t-p}$. We can learn $\\beta_{[1, p]}$ and $\\alpha$ using a linear regression defined by:\n",
        "\n",
        "$$\n",
        "[\\alpha, \\beta_{[1, p]}] = X \\cdot X_{lags}^\\intercal \\cdot (X_{lags} \\cdot X_{lags}^\\intercal)^{-1}\n",
        "$$\n",
        "\n",
        "where $X$ is the whole timeseries with an available lag ($t-p$ timesteps have $p$ past values, the $p$ first timesteps do not have pasts values), and $X_{lags}$ are the $X_{t-1}, \\dots, X_{t-p}$ for all time steps with an available lag $t-p$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS-H1S6v8J6H"
      },
      "source": [
        "# We store all values of the series in a numpy array called series\n",
        "series = df[\"Temp\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QgU0UMW0sYJ"
      },
      "source": [
        "def auto_regression(series, order):\n",
        "    \n",
        "    n_points = len(series)\n",
        "\n",
        "    # All lagged values will be stored in y_lag.\n",
        "    # If order is 7, for each timestep we will store 7 values.\n",
        "    X_lag = np.zeros((order, n_points-order))\n",
        "\n",
        "    # All current values will be stores in X.\n",
        "    X = np.zeros((1, n_points-order))\n",
        "    for i in range(0, n_points-order-1):\n",
        "        X_lag[:, i] = series[i:i+order]  # get the lagged values\n",
        "        X[:, i] = series[i+order+1]  # get the current value\n",
        "\n",
        "    # Add a constant term (c=1) to X_lag to compute alpha in the linear\n",
        "    # regression\n",
        "    X_lag = np.vstack((np.ones((1, n_points-order)), X_lag))\n",
        "\n",
        "    # Linear regression\n",
        "    coef = np.dot(np.dot(X, X_lag.T), scipy.linalg.pinv(np.dot(X_lag, X_lag.T)))\n",
        "    \n",
        "    alpha = coef[:, 0]\n",
        "    beta = coef[:, 1:]\n",
        "\n",
        "    return alpha, beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSzaWPLBVqNP"
      },
      "source": [
        "alpha, beta = auto_regression(series, order=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLhElljhjDcN"
      },
      "source": [
        "Now that we have our coefficients learned, we can make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1SpWhKH6dZ5"
      },
      "source": [
        "lag = beta.shape[1]\n",
        "\n",
        "Y_truth = []  # real timeseries\n",
        "Y_pred = []   # predictions\n",
        "for i in range(0, len(series)-lag-1):\n",
        "    # apply the equation of AR using the coefficients at each time steps\n",
        "    y = alpha + np.dot(beta, series[i:i+lag]) # y[t] = alpha + y[t-1]*beta1 + y[t-2]*beta2 + ...\n",
        "\n",
        "    Y_pred.append(y)\n",
        "    Y_truth.append(series[i+lag+1])\n",
        "\n",
        "Y_pred = np.array(Y_pred).flatten()\n",
        "Y_truth = np.array(Y_truth).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOI0P0tk-f4p"
      },
      "source": [
        "# Plot the results for one year\n",
        "plt.plot(series[lag+1:lag+366], label=\"True series\")\n",
        "plt.plot(Y_pred[:365], label=\"Predicted values\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xII3XgRbkGfI"
      },
      "source": [
        "And here are our coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN-7tPDiDTKn"
      },
      "source": [
        "coefs = np.c_[alpha, beta]\n",
        "plt.bar(np.arange(coefs.shape[1]), coefs.flatten())\n",
        "labels = ['$\\\\alpha$']\n",
        "for i in range(beta.shape[1]):\n",
        "    labels.append(f\"$\\\\beta_{i+1}$\")\n",
        "\n",
        "plt.xticks(np.arange(coefs.shape[1]), labels)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLNjb2-2FSnf"
      },
      "source": [
        "### 2.1 Modeling : ARIMA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0RRf-3JFTZP"
      },
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXPMMMivYZ5j"
      },
      "source": [
        "ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average, capturing the key aspects of the model :\n",
        "\n",
        "- **AR** : *AutoRegressive* A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
        "A pure AR model is such that :\n",
        "$$\n",
        "Y_t = \\alpha + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + ... + \\beta_{p} Y_{t-p} + \\epsilon_1\n",
        "$$\n",
        "- **I** : *Integrated* The use of differencing of raw observations in order to make the time series stationary\n",
        "- **MA** : *Moving Average* A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations\n",
        "A pure moving average model is such that :\n",
        "$$\n",
        "Y_t = \\alpha + \\epsilon_t + \\phi_1 \\epsilon_{t-1} + \\phi_2 \\epsilon_{t-2} + ... + \\phi_q \\epsilon_{t-q}\n",
        "$$\n",
        "\n",
        "\n",
        "Thus finally, the equation for ARIMA becomes :\n",
        "$$\n",
        "Y_t = \\alpha + \\beta_1 Y_{t-1} + ... + \\beta_p Y_{t-p} \\epsilon_t + \\phi_1 \\epsilon_{t-1} + ... + \\phi_q \\epsilon_{t-q} \n",
        "$$\n",
        "\n",
        "Each of these components is specified in the model as a parameter :\n",
        "- **p** : number of lag observations\n",
        "- **d** : number of times that raw observations are differenced. \n",
        "It is the minimum number of differencing needed to make the series stationary. If the time series is already stationary, then d= 0\n",
        "- **q** : size of moving average window\n",
        "\n",
        "Now, we will fit an ARIMA forecast model to the daily minimum temperature data.\n",
        "The data contains a one-year seasonal component :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60XBt5UVTgEL"
      },
      "source": [
        "# seasonal difference\n",
        "differenced = df.diff(365) \n",
        "# trim off the first year of empty data\n",
        "differenced = differenced[365:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuHItgDnzlz7"
      },
      "source": [
        "# Create an ARIMA model (check the statsmodels docs)\n",
        "model = ...\n",
        "\n",
        "# fit model\n",
        "model_fit = model.fit()\n",
        "print(model_fit.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7PBGeL4z2X-"
      },
      "source": [
        "# reviewing the residual errors\n",
        "# line plot\n",
        "residuals = pd.DataFrame(model_fit.resid)\n",
        "residuals.plot()\n",
        "plt.show()\n",
        "# density plot\n",
        "residuals.plot(kind='kde')\n",
        "plt.show()\n",
        "# summary stats\n",
        "print(\"Residuals stats:\", residuals.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb0pyLGz122p"
      },
      "source": [
        "To evaluate the ARIMA model, we will use walk forward validation. First we split the data into a training and testing set (initially, a year is a good interval to test for this dataset given the seasonal nature). \n",
        "A model will be constructed on the historic data and predict the next time step. The real observation of the time step will be added to the history, a new model constructed and the next time step predicted. \n",
        "The forecasts will be collected together to the final observations to give an error score (for example, RSME : root mean square error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAcF4hsM1c2B"
      },
      "source": [
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# rolling forecast with ARIMA\n",
        "\n",
        "train, test = differenced.iloc[:-365], differenced.iloc[-365:]\n",
        "\n",
        "# walk-forward validation\n",
        "values = train.values\n",
        "history = [v for v in values]\n",
        "predictions = list()\n",
        "test_values = test.values\n",
        "for t in range(len(test_values)):\n",
        "  # fit model\n",
        "  model = ARIMA(history, order=(7,0,0))\n",
        "  model_fit = model.fit()\n",
        "  # make prediction \n",
        "  yhat = model_fit.forecast()[0]\n",
        "  predictions.append(yhat)\n",
        "  history.append(test_values[t])\n",
        "\n",
        "# evaluate forecast\n",
        "rsme = sqrt(mean_squared_error(test_values, predictions))\n",
        "print('RSME : ', rsme)\n",
        "\n",
        "# plot forecasts against actual outcomes\n",
        "plt.plot(test)\n",
        "plt.plot(predictions, color='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWOOyUYM-JzC"
      },
      "source": [
        "We can also use the predict() function on the results object to make predictions. It accepts the index of the time steps to make predictions as arguments. These indexes are relative to the start of the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqfI5_Dw8mZg"
      },
      "source": [
        "forecast = model_fit.predict(start=len(train.values), end=len(differenced.values), typ='levels')\n",
        "plt.plot(test)\n",
        "plt.plot(forecast, color='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5Jldf6ETgZV"
      },
      "source": [
        "## Exercise: Mauna Loa CO<sub>2</sub> concentration levels (1975 - 2021)\n",
        "\n",
        "\n",
        "Carbon dioxyde (CO<sub>2</sub>) is a gas naturaly present in our environment. However, the concentration of CO<sub>2</sub> is increasing every year, mainly because of human activities. It is one of the major cause of global warming, and its value is precautiounously measured since 1973 at the Mauna Loa observatory, in Hawaii.\n",
        "\n",
        "We will get interested on the measures performed between 1975 and 2021. The dataset is composed of monthly averaged values. Values are expressed in *ppm* (parts-per-million).\n",
        "\n",
        "**Question: Appliying the method described above, model the behaviour of this timeseries.**\n",
        "\n",
        "**Question: Using your model, make predictions from 2001 to 2021, and evaluate the performance of your model. Make some projections about the evolution of the concentration after 2021.**\n",
        "\n",
        "**Do not forget to explain your hypotheses, choices and results.**\n",
        "\n",
        "*Some help*\n",
        "\n",
        "- Be careful ! This timeseries is more difficult to model (do not forget the stationarity property...)\n",
        "- If a timeseries is not stationary, one can **differenciate** its values over time to create a stationary approximation of the timeseries (like ARIMA does). You can also **remove the linear trend** from the data. Differencing (for an order 1 differenciation) implies transforming $X[t]$ into $X[t] - X[t-1]$.\n",
        "- Maybe a seasonal model (SARIMA, ...) could be interesting ?\n",
        "- You can do projections by using the model as a **generative model**: using the predicted value $X[t]$, you can predict $X[t+1$] using $X[t]$, then predict $X[t+2]$ using $X[t+1]$ and so on, using only the predictions of your model. For instance, with a dataset stopping in December 2021, you can predict January 2022 using December 2021, which you know from the dataset. Then, you can predict February 2022 from January 2022, March 2022 from February 2022...\n",
        "\n",
        "*Reference:*\n",
        "\n",
        "K.W. Thoning, A.M. Crotwell, and J.W. Mund (2021), Atmospheric Carbon Dioxide Dry Air Mole Fractions from continuous measurements at Mauna Loa, Hawaii, Barrow, Alaska, American Samoa and South Pole. 1973-2020, Version 2021-08-09 National Oceanic and Atmospheric Administration (NOAA), Global Monitoring Laboratory (GML), Boulder, Colorado, USA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoJIl7JfGUT5"
      },
      "source": [
        "ts = pd.read_csv(\"https://gml.noaa.gov/aftp/data/trace_gases/co2/in-situ/surface/mlo/co2_mlo_surface-insitu_1_ccgg_MonthlyData.txt\",\n",
        "                 header=150, sep=\" \")\n",
        "\n",
        "ts = ts[ts[\"year\"] > 1975]\n",
        "\n",
        "time_index = pd.DatetimeIndex(pd.to_datetime(ts[[\"year\", \"month\", \"day\"]]))\n",
        "ts = ts.set_index(time_index)\n",
        "ts = pd.Series(ts[\"value\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXFqWTLORzDK"
      },
      "source": [
        "ts.plot(figsize=(10, 5))\n",
        "\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"CO2 (ppm)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sEIrk-oo5io"
      },
      "source": [
        "***(Your code and explanations here and below)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-iegnFWqkCj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}